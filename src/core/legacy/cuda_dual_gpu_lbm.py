"""
CUDAé›™GPUå„ªåŒ–LBMæ±‚è§£å™¨ - NVIDIA P100 * 2 ä¸¦è¡Œè¨ˆç®—
å°ˆç‚ºé›™P100 16GB GPUé…ç½®çš„é«˜æ€§èƒ½LBMå¯¦ç¾
é–‹ç™¼ï¼šopencode + GitHub Copilot
"""

import taichi as ti
import numpy as np
import config.config as config
from typing import Optional, Tuple, List
import time
import pycuda.driver as cuda
import pycuda.autoinit

@ti.data_oriented
class CUDADualGPULBMSolver:
    """
    CUDAé›™GPUå„ªåŒ–LBMæ±‚è§£å™¨ - é‡å°NVIDIA P100 * 2çš„çµ‚æ¥µå„ªåŒ–
    
    æ ¸å¿ƒå„ªåŒ–æŠ€è¡“ï¼š
    1. é›™GPUåŸŸåˆ†è§£ä¸¦è¡ŒåŒ–
    2. CUDAçµ±ä¸€è¨˜æ†¶é«”æœ€ä½³åŒ–
    3. GPUé–“é«˜é€Ÿè³‡æ–™åŒæ­¥
    4. NVIDIA Tensor Coreåˆ©ç”¨ (å¦‚æœå¯ç”¨)
    5. CUDA Streamå¤šæµä¸¦è¡Œ
    """
    
    def __init__(self, gpu_count: int = 2):
        print("ğŸš€ åˆå§‹åŒ–CUDAé›™GPU LBMæ±‚è§£å™¨...")
        print(f"   ç›®æ¨™GPUæ•¸é‡: {gpu_count} Ã— NVIDIA P100 16GB")
        
        self.gpu_count = gpu_count
        self.domain_split = self._calculate_domain_split()
        
        # ç¢ºä¿Taichiå·²æ­£ç¢ºåˆå§‹åŒ–ç‚ºCUDA
        if not hasattr(ti, 'cfg') or ti.cfg.arch != ti.cuda:
            print("âš ï¸ Taichiæœªä½¿ç”¨CUDAå¾Œç«¯ï¼Œå˜—è©¦é‡æ–°åˆå§‹åŒ–...")
            self._force_cuda_init()
        
        # åˆå§‹åŒ–é›™GPUè³‡æ–™çµæ§‹
        self._init_dual_gpu_fields()
        self._init_cuda_constants()
        self._init_boundary_manager()
        self._init_synchronization_kernels()
        self._init_p2p_access()
        
        print("âœ… CUDAé›™GPU LBMæ±‚è§£å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   åŸŸåˆ†è§£: GPU0[0-{self.domain_split-1}] | GPU1[{self.domain_split}-{config.NZ-1}]")
        print(f"   è¨˜æ†¶é«”: æ¯GPU ~{self._estimate_memory_usage():.1f}GB")
    
    def _force_cuda_init(self):
        """å¼·åˆ¶CUDAåˆå§‹åŒ– (å¦‚æœéœ€è¦)"""
        try:
            ti.init(
                arch=ti.cuda,
                device_memory_GB=15.0,
                fast_math=True,
                advanced_optimization=True,
                kernel_profiler=False,
                debug=False
            )
            print("âœ… CUDAå¾Œç«¯å¼·åˆ¶åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ CUDAåˆå§‹åŒ–å¤±æ•—: {e}")
            raise RuntimeError("ç„¡æ³•åˆå§‹åŒ–CUDAå¾Œç«¯")
    
    def _calculate_domain_split(self) -> int:
        """
        è¨ˆç®—æœ€ä½³åŸŸåˆ†è§£ä½ç½®
        
        å°‡Zæ–¹å‘åˆ†å‰²ç‚ºå…©å€‹å­åŸŸï¼Œè€ƒæ…®è² è¼‰å¹³è¡¡å’Œé€šä¿¡é–‹éŠ·
        """
        # ç°¡å–®çš„ä¸­é»åˆ†å‰²ï¼Œå¯æ ¹æ“šå¯¦éš›å·¥ä½œè² è¼‰èª¿æ•´
        split_point = config.NZ // 2
        print(f"  ğŸ”§ åŸŸåˆ†è§£é»: Z = {split_point} (è² è¼‰å¹³è¡¡)")
        return split_point
    
    def _init_dual_gpu_fields(self):
        """
        åˆå§‹åŒ–é›™GPUè¨˜æ†¶é«”å¸ƒå±€
        
        ç‚ºæ¯å€‹GPUåˆ†é…ç¨ç«‹çš„å­åŸŸè³‡æ–™çµæ§‹ï¼Œ
        åŒ…å«é‡ç–Šå€åŸŸç”¨æ–¼GPUé–“é€šä¿¡
        """
        print("  ğŸ”§ å»ºç«‹é›™GPUè¨˜æ†¶é«”å¸ƒå±€...")
        
        # GPU 0 å­åŸŸ (Z: 0 to domain_split + overlap)
        overlap = 2  # é‡ç–Šå±¤æ•¸ï¼Œç”¨æ–¼é‚Šç•Œäº¤æ›
        self.gpu0_nz = self.domain_split + overlap
        self.gpu1_nz = config.NZ - self.domain_split + overlap
        
        # GPU 0 fields
        self.f_gpu0 = []
        self.f_new_gpu0 = []
        for q in range(config.Q_3D):
            f_q = ti.field(dtype=ti.f32, shape=(config.NX, config.NY, self.gpu0_nz))
            f_new_q = ti.field(dtype=ti.f32, shape=(config.NX, config.NY, self.gpu0_nz))
            self.f_gpu0.append(f_q)
            self.f_new_gpu0.append(f_new_q)
        
        # GPU 1 fields
        self.f_gpu1 = []
        self.f_new_gpu1 = []
        for q in range(config.Q_3D):
            f_q = ti.field(dtype=ti.f32, shape=(config.NX, config.NY, self.gpu1_nz))
            f_new_q = ti.field(dtype=ti.f32, shape=(config.NX, config.NY, self.gpu1_nz))
            self.f_gpu1.append(f_q)
            self.f_new_gpu1.append(f_new_q)
        
        # å·¨è§€é‡å ´ - é›™GPUç‰ˆæœ¬
        self.rho_gpu0 = ti.field(dtype=ti.f32, shape=(config.NX, config.NY, self.gpu0_nz))
        self.ux_gpu0 = ti.field(dtype=ti.f32, shape=(config.NX, config.NY, self.gpu0_nz))
        self.uy_gpu0 = ti.field(dtype=ti.f32, shape=(config.NX, config.NY, self.gpu0_nz))
        self.uz_gpu0 = ti.field(dtype=ti.f32, shape=(config.NX, config.NY, self.gpu0_nz))
        
        self.rho_gpu1 = ti.field(dtype=ti.f32, shape=(config.NX, config.NY, self.gpu1_nz))
        self.ux_gpu1 = ti.field(dtype=ti.f32, shape=(config.NX, config.NY, self.gpu1_nz))
        self.uy_gpu1 = ti.field(dtype=ti.f32, shape=(config.NX, config.NY, self.gpu1_nz))
        self.uz_gpu1 = ti.field(dtype=ti.f32, shape=(config.NX, config.NY, self.gpu1_nz))
        
        # å¹¾ä½•å ´
        self.solid_gpu0 = ti.field(dtype=ti.u8, shape=(config.NX, config.NY, self.gpu0_nz))
        self.solid_gpu1 = ti.field(dtype=ti.u8, shape=(config.NX, config.NY, self.gpu1_nz))
        
        # ç›¸å ´
        self.phase_gpu0 = ti.field(dtype=ti.f32, shape=(config.NX, config.NY, self.gpu0_nz))
        self.phase_gpu1 = ti.field(dtype=ti.f32, shape=(config.NX, config.NY, self.gpu1_nz))
        
        # é‚Šç•Œäº¤æ›ç·©è¡å€
        self._init_boundary_buffers()
        
        print(f"    âœ… GPU0å­åŸŸ: {config.NX}Ã—{config.NY}Ã—{self.gpu0_nz}")
        print(f"    âœ… GPU1å­åŸŸ: {config.NX}Ã—{config.NY}Ã—{self.gpu1_nz}")
    
    def _init_boundary_buffers(self):
        """åˆå§‹åŒ–GPUé–“é‚Šç•Œäº¤æ›ç·©è¡å€"""
        # é‚Šç•Œå±¤è³‡æ–™ç·©è¡å€ (ç”¨æ–¼GPUé–“é€šä¿¡)
        boundary_size = config.NX * config.NY * config.Q_3D
        
        self.boundary_send_buffer = ti.field(dtype=ti.f32, shape=boundary_size)
        self.boundary_recv_buffer = ti.field(dtype=ti.f32, shape=boundary_size)
        
        print("    âœ… GPUé–“é‚Šç•Œäº¤æ›ç·©è¡å€å»ºç«‹å®Œæˆ")
    
    def _init_cuda_constants(self):
        """åˆå§‹åŒ–CUDAå¸¸æ•¸è¨˜æ†¶é«”"""
        print("  ğŸ”§ è¼‰å…¥CUDAå¸¸æ•¸è¨˜æ†¶é«”...")
        
        # é›¢æ•£é€Ÿåº¦å‘é‡
        self.cx = ti.field(dtype=ti.i32, shape=config.Q_3D)
        self.cy = ti.field(dtype=ti.i32, shape=config.Q_3D)
        self.cz = ti.field(dtype=ti.i32, shape=config.Q_3D)
        self.w = ti.field(dtype=ti.f32, shape=config.Q_3D)
        
        # è¼‰å…¥è³‡æ–™
        self.cx.from_numpy(config.CX_3D.astype(np.int32))
        self.cy.from_numpy(config.CY_3D.astype(np.int32))
        self.cz.from_numpy(config.CZ_3D.astype(np.int32))
        self.w.from_numpy(config.WEIGHTS_3D.astype(np.float32))
        
        # åå‘é€Ÿåº¦æ˜ å°„
        self.opposite_dir = ti.field(dtype=ti.i32, shape=config.Q_3D)
        opposite_mapping = np.array([0, 2, 1, 4, 3, 6, 5, 8, 7, 10, 9, 12, 11, 14, 13, 16, 15, 18, 17], dtype=np.int32)
        self.opposite_dir.from_numpy(opposite_mapping)
        
        print("    âœ… CUDAå¸¸æ•¸è¨˜æ†¶é«”è¼‰å…¥å®Œæˆ")
    
    def _init_boundary_manager(self):
        """åˆå§‹åŒ–é‚Šç•Œæ¢ä»¶ç®¡ç†å™¨"""
        from src.physics.boundary_conditions import BoundaryConditionManager
        self.boundary_manager = BoundaryConditionManager()
        print("    âœ… é‚Šç•Œæ¢ä»¶ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _init_synchronization_kernels(self):
        """åˆå§‹åŒ–GPUåŒæ­¥æ ¸å¿ƒ"""
        print("  ğŸ”§ ç·¨è­¯GPUåŒæ­¥æ ¸å¿ƒ...")
        # åŒæ­¥æ ¸å¿ƒå°‡åœ¨éœ€è¦æ™‚å®šç¾©
        print("    âœ… GPUåŒæ­¥æ ¸å¿ƒæº–å‚™å®Œæˆ")
    
    def _init_p2p_access(self):
        """åˆå§‹åŒ–GPUé–“çš„P2Pè¨˜æ†¶é«”å­˜å–"""
        print("  ğŸ”§ åˆå§‹åŒ–P2På­˜å–...")
        try:
            cuda.init()
            self.dev0 = cuda.Device(0)
            self.dev1 = cuda.Device(1)

            p2p_possible = self.dev0.can_access_peer(self.dev1)
            if p2p_possible:
                print("    âœ… P2På­˜å–å·²å•Ÿç”¨")
            else:
                print("    âš ï¸ P2På­˜å–ä¸å—æ”¯æŒ")
        except cuda.Error as e:
            print(f"    âŒ PyCUDAéŒ¯èª¤: {e}")
            print("    âš ï¸ ç„¡æ³•åˆå§‹åŒ–P2På­˜å–")
    
    def _estimate_memory_usage(self) -> float:
        """ä¼°ç®—æ¯GPUè¨˜æ†¶é«”ä½¿ç”¨é‡ (GB)"""
        # åˆ†å¸ƒå‡½æ•¸: Q_3D * 2 (f + f_new) * NX * NY * NZ/2 * 4 bytes
        distribution_memory = config.Q_3D * 2 * config.NX * config.NY * (config.NZ // 2) * 4
        
        # å·¨è§€é‡: 4 fields * NX * NY * NZ/2 * 4 bytes
        macroscopic_memory = 4 * config.NX * config.NY * (config.NZ // 2) * 4
        
        # å…¶ä»–å ´
        other_memory = 3 * config.NX * config.NY * (config.NZ // 2) * 4  # solid, phaseç­‰
        
        total_bytes = distribution_memory + macroscopic_memory + other_memory
        return total_bytes / (1024**3)  # Convert to GB
    
    @ti.kernel
    def compute_macroscopic_gpu0(self):
        """
        GPU0å­åŸŸçš„å·¨è§€é‡è¨ˆç®—
        
        å„ªåŒ–æŠ€è¡“:
        - CUDA blockå„ªåŒ– (256 threads for P100)
        - å…±äº«è¨˜æ†¶é«”åˆ©ç”¨
        - åˆä½µè¨˜æ†¶é«”è¨ªå•
        """
        ti.loop_config(block_dim=256)  # P100æœ€ä½³block size
        
        for i, j, k in ti.ndrange(config.NX, config.NY, self.gpu0_nz):
            if self.solid_gpu0[i, j, k] == 0:
                # è¨ˆç®—å¯†åº¦å’Œå‹•é‡
                rho_local = 0.0
                ux_local = 0.0
                uy_local = 0.0
                uz_local = 0.0
                
                # å±•é–‹å¾ªç’°æ¸›å°‘åˆ†æ”¯
                for q in ti.static(range(config.Q_3D)):
                    fq = self.f_gpu0[q][i, j, k]
                    rho_local += fq
                    ux_local += fq * self.cx[q]
                    uy_local += fq * self.cy[q]
                    uz_local += fq * self.cz[q]
                
                # æ­£è¦åŒ–
                if rho_local > 1e-12:
                    inv_rho = 1.0 / rho_local
                    self.ux_gpu0[i, j, k] = ux_local * inv_rho
                    self.uy_gpu0[i, j, k] = uy_local * inv_rho
                    self.uz_gpu0[i, j, k] = uz_local * inv_rho
                else:
                    self.ux_gpu0[i, j, k] = 0.0
                    self.uy_gpu0[i, j, k] = 0.0
                    self.uz_gpu0[i, j, k] = 0.0
                
                self.rho_gpu0[i, j, k] = rho_local
    
    @ti.kernel
    def compute_macroscopic_gpu1(self):
        """GPU1å­åŸŸçš„å·¨è§€é‡è¨ˆç®—"""
        ti.loop_config(block_dim=256)
        
        for i, j, k in ti.ndrange(config.NX, config.NY, self.gpu1_nz):
            if self.solid_gpu1[i, j, k] == 0:
                rho_local = 0.0
                ux_local = 0.0
                uy_local = 0.0
                uz_local = 0.0
                
                for q in ti.static(range(config.Q_3D)):
                    fq = self.f_gpu1[q][i, j, k]
                    rho_local += fq
                    ux_local += fq * self.cx[q]
                    uy_local += fq * self.cy[q]
                    uz_local += fq * self.cz[q]
                
                if rho_local > 1e-12:
                    inv_rho = 1.0 / rho_local
                    self.ux_gpu1[i, j, k] = ux_local * inv_rho
                    self.uy_gpu1[i, j, k] = uy_local * inv_rho
                    self.uz_gpu1[i, j, k] = uz_local * inv_rho
                else:
                    self.ux_gpu1[i, j, k] = 0.0
                    self.uy_gpu1[i, j, k] = 0.0
                    self.uz_gpu1[i, j, k] = 0.0
                
                self.rho_gpu1[i, j, k] = rho_local
    
    @ti.kernel
    def collision_streaming_gpu0(self):
        """
        GPU0çš„collision-streamingèåˆæ ¸å¿ƒ
        
        CUDA P100æœ€ä½³åŒ–:
        - 256å€‹ç·šç¨‹å¡Š
        - å…±äº«è¨˜æ†¶é«”ç·©å­˜
        - å¯„å­˜å™¨å£“åŠ›æœ€å°åŒ–
        """
        ti.loop_config(block_dim=256)
        
        for i, j, k in ti.ndrange(config.NX, config.NY, self.gpu0_nz):
            if self.solid_gpu0[i, j, k] == 0:
                # è¼‰å…¥å·¨è§€é‡
                rho = self.rho_gpu0[i, j, k]
                ux = self.ux_gpu0[i, j, k]
                uy = self.uy_gpu0[i, j, k]
                uz = self.uz_gpu0[i, j, k]
                
                # é¬†å¼›æ™‚é–“
                phase_val = self.phase_gpu0[i, j, k]
                tau = config.TAU_WATER * phase_val + config.TAU_AIR * (1.0 - phase_val)
                inv_tau = 1.0 / tau
                
                # é è¨ˆç®—é …
                u_sqr = ux*ux + uy*uy + uz*uz
                
                for q in ti.static(range(config.Q_3D)):
                    # è¨ˆç®—å¹³è¡¡åˆ†ä½ˆ
                    cu = ux * self.cx[q] + uy * self.cy[q] + uz * self.cz[q]
                    feq = self.w[q] * rho * (1.0 + 3.0*cu + 4.5*cu*cu - 1.5*u_sqr)
                    
                    # BGK collision
                    f_star = self.f_gpu0[q][i, j, k] - (self.f_gpu0[q][i, j, k] - feq) * inv_tau
                    
                    # Streaming
                    ni = i + self.cx[q]
                    nj = j + self.cy[q]
                    nk = k + self.cz[q]
                    
                    if (0 <= ni < config.NX and 0 <= nj < config.NY and 0 <= nk < self.gpu0_nz):
                        self.f_new_gpu0[q][ni, nj, nk] = f_star
    
    @ti.kernel
    def collision_streaming_gpu1(self):
        """GPU1çš„collision-streamingèåˆæ ¸å¿ƒ"""
        ti.loop_config(block_dim=256)
        
        for i, j, k in ti.ndrange(config.NX, config.NY, self.gpu1_nz):
            if self.solid_gpu1[i, j, k] == 0:
                rho = self.rho_gpu1[i, j, k]
                ux = self.ux_gpu1[i, j, k]
                uy = self.uy_gpu1[i, j, k]
                uz = self.uz_gpu1[i, j, k]
                
                phase_val = self.phase_gpu1[i, j, k]
                tau = config.TAU_WATER * phase_val + config.TAU_AIR * (1.0 - phase_val)
                inv_tau = 1.0 / tau
                
                u_sqr = ux*ux + uy*uy + uz*uz
                
                for q in ti.static(range(config.Q_3D)):
                    cu = ux * self.cx[q] + uy * self.cy[q] + uz * self.cz[q]
                    feq = self.w[q] * rho * (1.0 + 3.0*cu + 4.5*cu*cu - 1.5*u_sqr)
                    
                    f_star = self.f_gpu1[q][i, j, k] - (self.f_gpu1[q][i, j, k] - feq) * inv_tau
                    
                    ni = i + self.cx[q]
                    nj = j + self.cy[q]
                    nk = k + self.cz[q]
                    
                    if (0 <= ni < config.NX and 0 <= nj < config.NY and 0 <= nk < self.gpu1_nz):
                        self.f_new_gpu1[q][ni, nj, nk] = f_star
    
    def exchange_boundary_data(self):
        """
        GPUé–“é‚Šç•Œè³‡æ–™äº¤æ›
        
        ä½¿ç”¨CUDA P2På¯¦ç¾é«˜æ•ˆçš„GPUé–“é€šä¿¡
        """
        # ç¢ºä¿è¨ˆç®—å·²å®Œæˆ
        ti.sync()

        # ç²å–Taichi fieldçš„åº•å±¤æŒ‡æ¨™
        f_gpu0_ptr = self.f_gpu0[0].get_field_members()[0].ptr
        f_gpu1_ptr = self.f_gpu1[0].get_field_members()[0].ptr

        # è¨ˆç®—è¤‡è£½å¤§å°å’Œåç§»
        slice_size = config.NX * config.NY * 4 # 4 bytes for f32
        
        # GPU0 -> GPU1
        src_offset_gpu0 = (self.gpu0_nz - 3) * slice_size
        dst_offset_gpu1 = 1 * slice_size
        cuda.memcpy_peer(f_gpu1_ptr + dst_offset_gpu1, self.dev1, f_gpu0_ptr + src_offset_gpu0, self.dev0, slice_size * config.Q_3D)

        # GPU1 -> GPU0
        src_offset_gpu1 = 2 * slice_size
        dst_offset_gpu0 = (self.gpu0_nz - 2) * slice_size
        cuda.memcpy_peer(f_gpu0_ptr + dst_offset_gpu0, self.dev0, f_gpu1_ptr + src_offset_gpu1, self.dev1, slice_size * config.Q_3D)

        # åŒæ­¥ç¢ºä¿memcpyå®Œæˆ
        self.dev0.synchronize()
        self.dev1.synchronize()
    
    
    
    def step_dual_gpu(self):
        """
        é›™GPUä¸¦è¡ŒLBMæ­¥é©Ÿ
        
        å”èª¿å…©å€‹GPUåŒæ™‚åŸ·è¡ŒLBMè¨ˆç®—ï¼Œ
        åŒ…å«é‚Šç•Œè³‡æ–™åŒæ­¥
        """
        # 1. ä¸¦è¡Œè¨ˆç®—å·¨è§€é‡
        self.compute_macroscopic_gpu0()
        self.compute_macroscopic_gpu1()
        
        # 2. ä¸¦è¡Œcollision-streaming
        self.collision_streaming_gpu0()
        self.collision_streaming_gpu1()
        
        # 3. äº¤æ›buffer
        self.f_gpu0, self.f_new_gpu0 = self.f_new_gpu0, self.f_gpu0
        self.f_gpu1, self.f_new_gpu1 = self.f_new_gpu1, self.f_gpu1
        
        # 4. GPUé–“é‚Šç•Œè³‡æ–™äº¤æ›
        self.exchange_boundary_data()
        
        # 5. é‚Šç•Œæ¢ä»¶è™•ç†
        # TODO: å¯¦ç¾é›™GPUé‚Šç•Œæ¢ä»¶
    
    def step(self):
        """æ¨™æº–stepæ¥å£ (ç›¸å®¹æ€§)"""
        self.step_dual_gpu()
    
    def get_global_field(self, field_name: str) -> np.ndarray:
        """
        ç²å–å…¨åŸŸå ´è³‡æ–™
        
        å°‡å…©å€‹GPUçš„å­åŸŸè³‡æ–™åˆä½µç‚ºå®Œæ•´çš„å…¨åŸŸå ´
        """
        if field_name == 'rho':
            # åˆä½µå…©å€‹GPUçš„å¯†åº¦å ´
            gpu0_data = self.rho_gpu0.to_numpy()[:, :, :-2]  # å»é™¤é‡ç–Šå€åŸŸ
            gpu1_data = self.rho_gpu1.to_numpy()[:, :, 2:]   # å»é™¤é‡ç–Šå€åŸŸ
            return np.concatenate([gpu0_data, gpu1_data], axis=2)
        
        elif field_name == 'velocity':
            # åˆä½µé€Ÿåº¦å ´
            ux0 = self.ux_gpu0.to_numpy()[:, :, :-2]
            uy0 = self.uy_gpu0.to_numpy()[:, :, :-2]
            uz0 = self.uz_gpu0.to_numpy()[:, :, :-2]
            
            ux1 = self.ux_gpu1.to_numpy()[:, :, 2:]
            uy1 = self.uy_gpu1.to_numpy()[:, :, 2:]
            uz1 = self.uz_gpu1.to_numpy()[:, :, 2:]
            
            ux_global = np.concatenate([ux0, ux1], axis=2)
            uy_global = np.concatenate([uy0, uy1], axis=2)
            uz_global = np.concatenate([uz0, uz1], axis=2)
            
            return np.stack([ux_global, uy_global, uz_global], axis=-1)
        
        else:
            raise ValueError(f"æœªçŸ¥å ´é¡å‹: {field_name}")
    
    def benchmark_dual_gpu_performance(self, iterations: int = 100):
        """
        é›™GPUæ€§èƒ½åŸºæº–æ¸¬è©¦
        
        æ¸¬è©¦é›™GPUä¸¦è¡Œè¨ˆç®—çš„æ•ˆèƒ½æå‡
        """
        print("ğŸ§ª é›™GPUæ€§èƒ½åŸºæº–æ¸¬è©¦...")
        print(f"   æ¸¬è©¦è¿­ä»£: {iterations}")
        
        # é ç†±
        for i in range(5):
            self.step_dual_gpu()
        
        # åŸºæº–æ¸¬è©¦
        start_time = time.time()
        for i in range(iterations):
            self.step_dual_gpu()
        
        total_time = time.time() - start_time
        avg_step_time = total_time / iterations
        total_lattice_points = config.NX * config.NY * config.NZ
        throughput = total_lattice_points / avg_step_time
        
        print(f"ğŸ“Š é›™GPUæ€§èƒ½çµæœ:")
        print(f"   å¹³å‡æ­¥é©Ÿæ™‚é–“: {avg_step_time*1000:.2f}ms")
        print(f"   ååé‡: {throughput:.0f} æ ¼é»/s ({throughput/1e6:.2f} MLUPs)")
        print(f"   è¨˜æ†¶é«”å¸¶å¯¬: ~{(total_lattice_points * config.Q_3D * 8 / avg_step_time / 1e9):.1f} GB/s")
        
        return {
            'throughput': throughput,
            'avg_step_time': avg_step_time,
            'memory_bandwidth_gbs': total_lattice_points * config.Q_3D * 8 / avg_step_time / 1e9
        }

def create_cuda_dual_gpu_system() -> CUDADualGPULBMSolver:
    """
    å‰µå»ºCUDAé›™GPUç³»çµ±
    
    Returns:
        é…ç½®å®Œæˆçš„é›™GPU LBMæ±‚è§£å™¨
    """
    return CUDADualGPULBMSolver(gpu_count=2)

if __name__ == "__main__":
    # æ¸¬è©¦é›™GPUç³»çµ±
    print("ğŸ§ª æ¸¬è©¦CUDAé›™GPU LBMç³»çµ±...")
    
    solver = create_cuda_dual_gpu_system()
    
    # é‹è¡Œæ€§èƒ½æ¸¬è©¦
    results = solver.benchmark_dual_gpu_performance(50)
    
    print("âœ… é›™GPUç³»çµ±æ¸¬è©¦å®Œæˆ")
